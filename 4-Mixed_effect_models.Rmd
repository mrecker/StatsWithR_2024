# Mixed Effect Models (MEM)

One of the key assumptions of linear models is that the data is independent and identically distributed (i.i.d.). This means that our data sample is a true representation of the underlying population and that we have sampled without any (intentional or unintentional) bias. Sometimes this is not the case, however, meaning that there could be some systematic difference between some of our samples. We have come across a situation like this in the previous example, where samples taken during the rainy season showed a different pattern with temperature than those sampled during the off-season. We dealt with this by adding `rainfall` as a categorical variable into our model. This worked not only because there were only two possible levels for `rainfall` but also because we care about the identity of those levels, i.e. we wanted to estimate how incidence changes for `rainfall = yes` and `rainfall = no`. In other cases we might still want to account for underlying differences in our data by including a categorical variable we believe is having an influence but without caring about their identity. These variables are referred to *random effect* variables, as opposed to *fixed effect* variables that we considered so far. And models that include random and fixed effects are known as *mixed effect models* or *random effect models*.  

  - **Fixed effects** are (continuous or categorical) exploratory variables that we believe have an effect on the response and that we are interested in making conclusions about. Their effects is constant (fixed) across individuals.
  - **Random effects** are categorical variables with many levels that are themselves drawn from a population of possible levels. Although they are believed to have an influence on the response, we are not interested in their identity.

Here we only briefly touch upon mixed effect models and how to analyse them in R using the `lme4` package and by means of the `sleepstudy` dataset that is provided by `lme4`, which contains experimental data on the effect of sleep deprivation on reaction time. The data file has three variables

  - `Reaction`: average reaction time (ms)
  - `Days`: number of days of sleep deprivation
  - `Subject`: subject number on which observation was made

You can find out more about this study through the help function `?sleepstudy`. 

```{r, warning=FALSE, message=FALSE}
# load library for mixed effect modelling
library(lme4)

# get a summary of the sleep study dataset
summary(sleepstudy)

# get the number of study participants
print(paste0("Number of participants: ",length(unique(sleepstudy$Subject))))
```

From the data summary we see that there were 10 observations made for 18 individuals. Therefore, `Subject` fits our definition of a *random effect`: we believe that there is a difference between study participants but we do not care about the individuals themselves.

We can convince ourselves about the difference between study participants by plotting the data for two subjects:

```{r, fig.height=4, fig.width=7, echo = F}
sleepstudy %>%
    filter(Subject %in% c(308, 309)) %>%
    ggplot(aes(x = Days, y = Reaction)) +
    geom_line(col = 'red', linetype = 'dashed') +
    geom_point(size=2) +
    scale_x_continuous(breaks = 0:9) +  # make scale discrete
    facet_wrap(~Subject)
```
```{task}
Recreate the above graph but for all individuals in the study. 
```

```{solution}

``{r}
sleepstudy %>%
    filter(Subject %in% c(308, 309)) %>%
    ggplot(aes(x = Days, y = Reaction)) +
    geom_line(col = 'red', linetype = 'dashed') +
    geom_point(size=2) +
    scale_x_continuous(breaks = 0:9) +  # make scale discrete
    facet_wrap(~Subject)
``

```

$~$

How do we model this data? There are principally three approaches: (1) pool all the data and don't care about individual-level differences, (2) add `Subject` as a fixed effect as well as the interaction term `Days:Subject`, and (3) treat `Subject` as a *random effect*. Here we are only going to compare (1) and (3) and leave (2) as an exercise.

Before we start we need to make some minor modification to the data. First, we want to make sure that `Subject` is being treated like a categorical and not a continuous variable. Second, the first two days (day 0 and day 1) were for adaptation and training, meaning that sleep deprivation started counting at day 2 so we discard the first two days and then recode to make sure we start with day 0. 

```{r}
sleep <- sleepstudy %>%
    mutate(Subject = factor(Subject)) %>%
    mutate(Days = Days - 2) %>%
    filter(Days>=0)
```

The `lmer()` function (from the `lme4` package) uses a very similar syntax as the by now familiar `lm()` function but where the model formula now takes on the form
<center> `response ~ fixed1 + fixed2 + ... + (ran1 + ran2 + ...| ran_factor1) + ...` </center>

That is, fixed effects (and their interactions) are entered exactly as before. New is the presence of the term `(ran1 + ran2 + ...| ran_factor1)`, which represents the random effects associated with a particular random factor. For our dataset there are three possibilities for how the random effects might influence the response, given here together with the specific formula syntax:

  - *random intercepts only*: `Response ~ Days + (1 | Subject)`
  - *random slopes only*: `Response ~ Days + (0 Days | Subject)`
  - *random slope and intersect*: `Response ~ Days + (Days | Subject)`

The most reasonable model would be the last, which allows for both the intercept and the slope to vary between individuals. So let's fit this model

```{r}
fit_mm <- lmer(Reaction ~ Days + (Days|Subject), data = sleep)
summary(fit_mm)
```

The model summary now contains a lot more information. Let's highlight the two most important sections.

#### **Fixed effects** {-}

There are two fixed effects: `(Intercept)` and `Days` which tell us the average reaction time without any sleep deprivation and the increase in reaction time with each day of sleep deprivation. You will notice that compared to the simpler modelling framework, *P* values are not provided anymore, and there are various reasons as to why. However, if you wish to make statements regarding the statistical significance of these estimates, you can either look at the confidence intervals (based on parametric boostrapping)

```{r}
confint(fit_mm, level=0.95)
```

The ones you are most interested in are the two bottom ones, and as neither crosses 0 you can be sure that the effects are statistically significant.

The other option is to calculate *P* values from the *t* values (the underlying maths goes beyon the scope of this workshop)

```{r}
tvals <- fixef(fit_mm) / sqrt(diag(vcov(fit_mm)))
2 * (1 - pnorm(abs(tvals)))
```

This again provides strong evidence for rejecting the null hypotheses that sleep deprivation does not effect reaction time.

#### **Random effects** {-}

This part of the summary looks more unfamiliar and explaining each part of this is beyond this workshop, as it requires more indepth knowledge about variance and covariance matrices and stats in general. So the only part you might want to take a look at is the `Variance` of the two `Groups` entries `Subject` and `Residual`. This tells you how much of the total variance in your data is being absorbed by the random effects (on slope and intercept). In this case this amounts to $\sim 61$% (100% * (958.35 + 45.78)/(958.35 + 45.78 + 651.60)), meaning that inter-subject variation explain the majority of the 'noise' in your data. 

If we are interested in the estimated random effects, these can be pulled out using `ranef()` (similar to `fixef()` for accessing estimates of the fixed effects)

```{r}
ranef(fit_mm) 
```

```{task, title: "Tasks"}

  1. Fit a linear regression model to the pooled data ignoring the effect of `Subject` and compare the estimated effect sizes.
  2. Fit a linear regression model using `Subject` as a categorical fixed effect and compare the estimated effect sizes
  3. Fit a mixed effect model as before but assume that `Subject` only affects the intersect. How does this compare to the previous model with random slope and intersect?

```

<br>

## Model checking

We need to be just as conscious of testing the assumptions of mixed effects models as we are with any other model. These assumptions include:

  1. Within-group errors are independent and normally distributed with mean zero and variance  
  2. Within-group errors are independent of the random effects
  3. Random effects are normally distributed with mean zero

Two commonly used plots to check these assumptions are:

  1. A simple plot of residuals against fitted values, irrespective of random effects (note we have to do this “by hand” here):
  ```{r}
  plot(fit_mm)
  ```

  2. Plotting the residuals against the fitted values, separately for each level of the random effect, using the `coplot()` function:
  ```{r, fig.height=7}
  coplot(resid(fit_mm) ~ fitted(fit_mm) | sleep$Subject)
  ```

## Further reading 

Due to time constraints we cannot go into more details of *random* or *mixed effects models*. The interested reader will find the following (free) resources useful:

  - [Introduction to Linear Mixed Models](https://ourcodingclub.github.io/tutorials/mixed-models/)
  - [Beyond Multiple Linear Regression](https://bookdown.org/roback/bookdown-BeyondMLR/)
  - [Mixed Models with R](https://m-clark.github.io/mixed-models-with-R/random_intercepts.html)
  
