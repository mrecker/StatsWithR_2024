[["index.html", "Introduction to statistical data analysis in R About Workshop structure", " Introduction to statistical data analysis in R Mario Recker About This workshop is intended to introduce the R/RStudio statistical programming environment and how to use it for efficient data handling, data visualisation and data analysis. It is aimed at individuals with little experience in R/Rstudio and basic statistics. For those who are interested in a general intro to R or a refresher, please have a look at the following site for a great introduction to R/RStudio Introduction to R. Equally, much of the material on statistical data analysis are based on the Statistical Modelling in R workshop. These workshops had been developed by TJ McKinley from the University of Exeter and JJ Valletta (\\(\\dagger\\)). Workshop structure This workshop will loosely be structured into the following themes Data handling importing data data types missing data Descriptive statistics summary statistics contingency tables graphical representation Inferential statistics linear models mixed effect models generalised linear models survivial analysis Latent Class Analysis \\(~\\) The workshop will consist of a mixture between (theoretical) background and hands-on practicals. Specific tasks, designed for you to test your new skills and understanding, are highlighted as Task This task will be explained here The solution can be revealed by clicking Show: Solution Solution A solution will appear here. However, please make sure you try before clicking on the Solution - this is the only way to make sure how fully understood the new concepts. \\(~\\) Prerequisites It is expected that you feel comfortable with basic R / RStudio and, specifically, the use of data.frame objects. The Appendix provides extra background material, covering a lot of necessary background. Also, for this workshop we will be relying on the tidyverse package, or rather suite of packages for performing a (growing) number of data science tasks, which include ggplot2 and dplyr. Please have a look in the Appendix for a brief introduction to their uses. \\(~\\) Important There will be plenty of time to practice your new skills, and please feel to aks if anything is not 100% clear. "],["data-handling.html", "1 Data handling Importing data Data types Missing data", " 1 Data handling Importing data An essential part of data analysis is the ability to read (and store) external datasets. There are a myriad of different ways this can be done in R, partially depending on the file type / software originally used to store the data. Here we focus on three different file formats: CSV files (filename.csv) Excel files (filename.xls or filename.xlsx) R data format (filename.rds or filename.RData) The main difference between the approaches is that an Rds file stores data in a (R specific) binary format, meaning that you need R to read it. The same goes for Excel files. CSV files, in comparison, are (human readable) text files that can be opened by a number of different programs. In fact, on most operating systems, these are opened by Excel or a text editor by default. The decision which approach you should use often depends on whether you are the only one working with the data or whether you like the added benefit of viewing (or manipulating) data outside the R environment. Note: Excel is known to occasionally cause havoc with data (mostly due to it changing data types by itself), we are urging caution when handling Excel files. Always check the validity of your data before starting with your analysis! CSV files CSV (Comma-Separated Values) is a widely used data format separating values with commas or other delimiters (e.g. semi-colon or tab). There are different function pairs for reading CSV files, such as the inbuilt read.table() or read.csv(), and their equivalent functions for writing CSV files. Here we will use the read_csv() function, which is part of the readr package (included in tidyverse). Note: this function is strictly for comma-separated files, read_csv2() reads semicolon-separated and the read_delim() function reads files with any separating character; for full fucntion details, see [here] (https://www.rdocumentation.org/packages/readr/versions/2.1.5/topics/read_delim). As an example we will be using the tobacco.csv file that contains a simulated dataset on tobacco use and health of 1000 individuals. Please make sure that the file is contained in your current working directory. tobacco &lt;- read.csv(&#39;tobacco.csv&#39;) Check if this worked by printing out the first 5 rows head(tobacco, n = 5) ## gender age age.gr BMI smoker cigs.per.day diseased disease ## 1 M 75 71 + 29.50225 No 0 No &lt;NA&gt; ## 2 F 35 35-50 26.14989 No 0 Yes Neurological ## 3 F 70 51-70 27.53183 No 0 No &lt;NA&gt; ## 4 F 40 35-50 24.05832 No 0 No &lt;NA&gt; ## 5 F 75 71 + 22.77486 No 0 Yes Hearing ## samp.wgts ## 1 1.062500 ## 2 1.044177 ## 3 1.049383 ## 4 1.044177 ## 5 1.062500 In an equivalent fashion we can save data in CSV format using the write_csv() function. # make sure you do not overwrite the original data file!! write_csv(tobacco, &quot;myData.csv&quot;) Excel files Although we discourage the use of Excel for data handling, it is still one of the most common methods for storing data. We therefore briefly show you one method of how to read Excel files using the read_excel function from the readxl package. NOTE: although readxl will be installed as part of tidyverse, you will still need to load it explicitly, because it is not a core tidyverse package. # load library library(readxl) # read in excel data file tobaccoXLS &lt;- read_excel(&#39;tobacco.xlsx&#39;) # print first few rows head(tobaccoXLS, n = 5) ## # A tibble: 5 × 9 ## gender age age.gr BMI smoker cigs.per.day diseased disease samp.wgts ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 M 75 71 + 29.5022463… No 0 No NA 1.06 ## 2 F 35 35-50 26.1498941… No 0 Yes Neurol… 1.04 ## 3 F 70 51-70 27.5318291… No 0 No NA 1.05 ## 4 F 40 35-50 24.0583177… No 0 No NA 1.04 ## 5 F 75 71 + 22.7748622… No 0 Yes Hearing 1.06 This functions offers a great deal of flexibility, from specifying which sheet to import (by default it will import the first one), specifying the data range, or repairing column names for easier downstream analysis. # specify the name of the sheet and only load the first 100 rows and 7 columns tobaccoXLS &lt;- read_excel(&#39;tobacco.xlsx&#39;, sheet = &#39;tobacco_2&#39;, range = &#39;A1:G101&#39;) ## which is equivalent to # dat &lt;- read_excel(&#39;tobacco.xlsx&#39;, range = &#39;tobacco_2!A1:G101&#39;) # check the first 5 rows head(tobaccoXLS, n = 5) ## # A tibble: 5 × 7 ## gender age `age group` BMI smoker `cigarettes per day` diseased ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 M 75 71 + 29.5 No 0 No ## 2 F 35 35-50 26.1 No 0 Yes ## 3 F 70 51-70 27.5 No 0 No ## 4 F 40 35-50 24.1 No 0 No ## 5 F 75 71 + 22.8 No 0 Yes We can see that two column names contain spaces, which can make life a bit tricky during subsequent operations. For this, read_excel provides a .name_repair argument, that ensures column names will be syntactic (not containing special characters or reserved words) and work everywhere. (Note: this argument can also be used in the read_csv() function.) tobaccoXLS &lt;- read_excel(&#39;tobacco.xlsx&#39;, sheet = &#39;tobacco_2&#39;, range = &#39;A1:G101&#39;, .name_repair = &#39;universal&#39;) ## New names: ## • `age group` -&gt; `age.group` ## • `cigarettes per day` -&gt; `cigarettes.per.day` head(tobaccoXLS, n = 5) ## # A tibble: 5 × 7 ## gender age age.group BMI smoker cigarettes.per.day diseased ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 M 75 71 + 29.5 No 0 No ## 2 F 35 35-50 26.1 No 0 Yes ## 3 F 70 51-70 27.5 No 0 No ## 4 F 40 35-50 24.1 No 0 No ## 5 F 75 71 + 22.8 No 0 Yes A really handy cheatsheet for importing data with the tidyverse can be found here R data files To demonstrate the use of R data file format, we go the other way around and start by saving our tobacco dataframe as an R object. saveRDS(tobacco, file = &#39;tobacco.Rds&#39;) Note: the file will be saved in your current working directory. If you want to save it in a different folder you need to provide the full file path, e.g. saveRDS(df, file = '.../StatsWithR//Data/myData.Rds'). Loading the data is equally simple: # first we remove the tobacco data object to make sure everything works rm(tobacco) # load the data readRDS(&#39;tobacco.Rds&#39;) ## gender age age.gr BMI smoker cigs.per.day diseased disease ## 1 M 75 71 + 29.502246 No 0 No &lt;NA&gt; ## 2 F 35 35-50 26.149894 No 0 Yes Neurological ## 3 F 70 51-70 27.531829 No 0 No &lt;NA&gt; ## 4 F 40 35-50 24.058318 No 0 No &lt;NA&gt; ## 5 F 75 71 + 22.774862 No 0 Yes Hearing ## 6 M 38 35-50 21.464121 No 0 No &lt;NA&gt; ## 7 M 45 35-50 18.948411 No 0 No &lt;NA&gt; ## 8 &lt;NA&gt; 59 51-70 30.376773 No 0 No &lt;NA&gt; ## 9 F 29 18-34 21.408103 Yes 38 No &lt;NA&gt; ... At first sight this looks like it has worked. However, you might also have noticed that there is no newly created object in your global environment. This means, although R successfully loaded the data, it was not assigned to an object and essentially lost; this is one of the key differences between saveRDS/readRDS and save/load (see below)). We therefore need to assign the read-in data to an object # read data and assign to an object called tobacco tobacco &lt;- readRDS(&#39;tobacco.Rds&#39;) # see if it works head(tobacco) ## gender age age.gr BMI smoker cigs.per.day diseased disease ## 1 M 75 71 + 29.50225 No 0 No &lt;NA&gt; ## 2 F 35 35-50 26.14989 No 0 Yes Neurological ## 3 F 70 51-70 27.53183 No 0 No &lt;NA&gt; ## 4 F 40 35-50 24.05832 No 0 No &lt;NA&gt; ## 5 F 75 71 + 22.77486 No 0 Yes Hearing ## 6 M 38 35-50 21.46412 No 0 No &lt;NA&gt; ## samp.wgts ## 1 1.062500 ## 2 1.044177 ## 3 1.049383 ## 4 1.044177 ## 5 1.062500 ## 6 1.044177 An alternative way is the use of the Rdata file format. # save our dataframe in Rdata format save(tobacco, file = &quot;tobacco.Rdata&quot;) # remove from our environment rm(tobacco) # load the data load(&quot;tobacco.Rdata&quot;) As you will have noticed, in this case we do not need to explicitly assign the loaded data to object; instead, the original name of the dataframe will be used. Another advantage of using this format is that we can save multiple objects at once # save both dataframe in one file save(tobacco, tobaccoXLS, file = &#39;tobacco_datasets.Rda&#39;) # remove from environment rm(tobacco, tobaccoXLS) # load the data load(&#39;tobacco_datasets.Rda&#39;) One of the advantages of saving data in an R data format is that it is more space efficient. By default, saveRDS() (or save()) compresses the data, which can significantly shrink the size of the stored data. On the downside, it can also take a bit longer to load the data subsequently. Data types Although we assume that you are likely already familiar with different types of data, this is just a reminder of the most common data types in R numeric / integer character factor logical We have already come across some of them when we imported the tobacco data in the previous section. To check what types of data are contained in this dataset we can use the class() function; for example # get the data type of column &#39;gender&#39; class(tobacco$gender) ## [1] &quot;character&quot; # get the data type of column &#39;BMI&#39; class(tobacco$BMI) ## [1] &quot;numeric&quot; Instead of doing this for every column separately, we can apply this function across the whole dataframe with sapply sapply(tobacco, class) ## gender age age.gr BMI smoker cigs.per.day ## &quot;character&quot; &quot;integer&quot; &quot;character&quot; &quot;numeric&quot; &quot;character&quot; &quot;integer&quot; ## diseased disease samp.wgts ## &quot;character&quot; &quot;character&quot; &quot;numeric&quot; Here’s a brief overview of what these data types are and what they are generally used for. Numeric / integer A variable will be automatically stored as numeric data if the values are numbers or contain decimals. Depending on the function used to import data, a variable can also be assigned explicitly as integer if it does not contain any decimals. # define a vector of three numerical values x &lt;- c(4, 3, 5) # get the data type class(x) ## [1] &quot;numeric&quot; # compare to the imported data column &#39;age&#39; class(tobacco$age) ## [1] &quot;integer&quot; We can also force R to convert a numeric variable into an integer, and vice versa x &lt;- as.integer(x) class(x) ## [1] &quot;integer&quot; tobacco$age &lt;- as.numeric(tobacco$age) class(tobacco$age) ## [1] &quot;numeric&quot; Character Character is the general data type to store text (strings), such as words, sentences etc. # first example a &lt;- &quot;this is a character data type&quot; class(a) ## [1] &quot;character&quot; # second example b &lt;- &quot;1&quot; # enclosing a number in quotes will define it as a character type class(b) ## [1] &quot;character&quot; # type conversion from numeric to character c &lt;- 12.3 d &lt;- as.character(c) class(d) ## [1] &quot;character&quot; # automatic &#39;conversion&#39; v &lt;- c(1, 2, 3, &quot;4&quot;) class(v) ## [1] &quot;character&quot; Factor At a first glance, factor variables look very similar to character types. However, factors are used to denote categorical variables, i.e. those that can only take a (pre-determined) limited number of values. # convert v into a categorical variable f1 &lt;- factor(v) # printing it will already reveal the difference to v as the factor levels are provided alongside the values f1 ## [1] 1 2 3 4 ## Levels: 1 2 3 4 # this works also on numeric variables f2 &lt;- factor(c(1, 2, 3, 4)) f2 ## [1] 1 2 3 4 ## Levels: 1 2 3 4 Looking at our tobacco dataframe we can see that some columns denote categorical variables (gender, age group, smoker, diseased and disease) and should be converted accordingly, which can be done either using the function as.factor() or just factor(). Here we will use dplyr’s mutate() function to convert all five columns simultaneously. tobacco &lt;- tobacco %&gt;% mutate(gender = factor(gender), age.gr = factor(age.gr), smoker = factor(smoker), diseased = factor(diseased), disease = factor(disease)) # check if this worked sapply(tobacco, class) ## gender age age.gr BMI smoker cigs.per.day ## &quot;factor&quot; &quot;numeric&quot; &quot;factor&quot; &quot;numeric&quot; &quot;factor&quot; &quot;integer&quot; ## diseased disease samp.wgts ## &quot;factor&quot; &quot;factor&quot; &quot;numeric&quot; Logical Logical variables can only take on two values&gt; TRUE and FALSE (which can also be abbreviated as T and F, respectively). These are common data types when used with logical operation, e.g. x &lt;- 5 y &lt;- 7 x &gt; y # read is &#39;is x greater than y&#39; ## [1] FALSE or if(x &gt; y){ print(&quot;x is greater than y&quot;) }else{ print(&quot;x is not greater than y&quot;) } ## [1] &quot;x is not greater than y&quot; Furthermore, a logical TRUE can be numerically interpreted as 1 and a FALSE as 0. as.integer(x &lt; y) ## [1] 1 This fact can be used in a variety of cases, as shown in the example below (try to understand what exactly is going on) # define a vector with 15 elements v &lt;- c(2, 6, 4, 8, 7, 8, 4, 1, 5, 8, 2, 9, 11, 4, 7) # are the elements less than or equal to 5 v &lt;= 5 ## [1] TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE ## [13] FALSE TRUE FALSE # how many elements are less than or equal to 5 sum(v &lt;= 5) ## [1] 7 Missing data More often than not we will find ourselves confronted with datasets that are not complete. That is, some information might be missing for some of the variables, and we need to find a way to deal with those cases. In R, missing values are usually represented by NA. Note: when importing data we can specify the value or symbol used to represent missing data. We can test for the presence of missing values using the is.na() function v &lt;- c(1, 4, 2, NA, 8, 9) is.na(v) ## [1] FALSE FALSE FALSE TRUE FALSE FALSE which confirms that there is a missing number between elements 3 and 5. To find the exact location we could use which(is.na(v)) ## [1] 4 The is.na() function can also be applied to data frames, as shown below is.na(tobacco) ## gender age age.gr BMI smoker cigs.per.day diseased disease ## [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [2,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [4,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [5,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [7,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [8,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [9,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ... and in combination with some other functions can be used to get crucial information about the degree of missingness in our data # count the number of missing entries in each column and convert to percentage 100 * colSums(is.na(tobacco)) / nrow(tobacco) ## gender age age.gr BMI smoker cigs.per.day ## 2.2 2.5 2.5 2.6 0.0 3.5 ## diseased disease samp.wgts ## 0.0 77.8 0.0 The big question always comes down to how to deal with missing data. The easiest approach is to just delete / ignore those cases, and there are two commonly used ways of doing this, with na.omit() or complete.cases(). The first function returns an object with all missing values removed # create a new data frame based on tobacco with all rows removed that contain NAs tobacco_noNAs &lt;- na.omit(tobacco) head(tobacco_noNAs, 10) ## gender age age.gr BMI smoker cigs.per.day diseased disease ## 2 F 35 35-50 26.14989 No 0 Yes Neurological ## 5 F 75 71 + 22.77486 No 0 Yes Hearing ## 15 M 47 35-50 29.04873 Yes 21 Yes Musculoskeletal ## 16 M 76 71 + 23.13594 Yes 29 Yes Vision ## 20 M 73 71 + 26.73234 No 0 Yes Heart ## 22 F 28 18-34 23.52206 Yes 15 Yes Cancer ## 24 F 80 71 + 26.31107 No 0 Yes Hypertension ## 29 F 68 51-70 27.20262 No 0 Yes Musculoskeletal ## 32 F 71 71 + 26.82957 No 0 Yes Neurological ## 39 F 22 18-34 21.23634 Yes 34 Yes Heart ## samp.wgts ## 2 1.0441767 ## 5 1.0625000 ## 15 1.0441767 ## 16 1.0625000 ## 20 1.0625000 ## 22 0.8614232 ## 24 1.0625000 ## 29 1.0493827 ## 32 1.0625000 ## 39 0.8614232 The other option is to use the comple.cases() function, which returns a logical vector indicating which rows contain no missing values (returned as TRUE) complete.cases(tobacco) ## [1] FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE TRUE ## [25] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [49] TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [73] FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE ## [109] FALSE TRUE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ... and this can then be used to subset the data, i.e. we select only those rows that have full information for all variables tobacco_noNAs &lt;- tobacco[complete.cases(tobacco),] head(tobacco_noNAs, 10) ## gender age age.gr BMI smoker cigs.per.day diseased disease ## 2 F 35 35-50 26.14989 No 0 Yes Neurological ## 5 F 75 71 + 22.77486 No 0 Yes Hearing ## 15 M 47 35-50 29.04873 Yes 21 Yes Musculoskeletal ## 16 M 76 71 + 23.13594 Yes 29 Yes Vision ## 20 M 73 71 + 26.73234 No 0 Yes Heart ## 22 F 28 18-34 23.52206 Yes 15 Yes Cancer ## 24 F 80 71 + 26.31107 No 0 Yes Hypertension ## 29 F 68 51-70 27.20262 No 0 Yes Musculoskeletal ## 32 F 71 71 + 26.82957 No 0 Yes Neurological ## 39 F 22 18-34 21.23634 Yes 34 Yes Heart ## samp.wgts ## 2 1.0441767 ## 5 1.0625000 ## 15 1.0441767 ## 16 1.0625000 ## 20 1.0625000 ## 22 0.8614232 ## 24 1.0625000 ## 29 1.0493827 ## 32 1.0625000 ## 39 0.8614232 Although this works, there are two major issues with this approach data loss data bias If you compare the original dataset with the reduced one you will notice two things: first, it is much smaller (203 observations compared to 1000), which is a reduction by over 70% even though only one of the variables contains more than 4% missing data; and second, the data now only contains observation of those individuals who are diseased, which would obviously bias our downstream analysis. Omitting missing data should thus only be done in exceptional circumstances and with great caution. The safer, alternative strategies are either data imputation, which essentially tries to guess the missing values based on other information contained in the data (this is not covered as part of this workshop, but if you are interested you could have a look at this site), or simply ignoring the missing data and relying on the fact that most R functions have their own way of dealing with missing values; e.g. compare # sum over all elements in v produces NA sum(v) ## [1] NA with # telling sum() to ignore, or remove NA&#39;s, solves this issue sum(v, na.rm = TRUE) ## [1] 24 Beware: different functions have different default settings when it comes to handling NA’s; always look in the help file to make sure they do what you want them to do! "],["descriptive-statistics-in-r.html", "2 Descriptive statistics in R 2.1 Univariate analysis 2.2 Multivariate analysis", " 2 Descriptive statistics in R The first thing we need to do is clarify the distinction between descriptive and inferential statistics. In simple terms, the former deals with the quantitative and/or visual description of a sample taken from the population, whereas the latter tries to make inferences about the properties of a population based on this sample. And this is an important point to remember: we usually never know the entire state of the population (or the underlying probability distribution that generated the data), we only have access to a (usually significantly smaller) sample of it. And statistics will help us to get a sense of how well this sample might represent the population and the uncertainty attached to any inferences we wish to make about it. 2.1 Univariate analysis As defined earlier, descriptive statistics is concerned with the quantitative and/or visual description of a data sample. We can broadly distinguish two cases: a univariate analysis, where we describe a single variable, for example by its central tendency (e.g. mean or median) and dispersal (e.g. variance or quartiles) multivariate analysis, where our data sample consist of more than one variable and where we are interested in the relationship between pairs of variables (e.g. correlation or covariance). Here we will introduce you the concept univariate analysis, and you are probably already familiar with most of the material dealt with in this section. We therefore only briefly touch upon some of them, showing how to do generate descriptive statistics in R - first by hand and then using some nifty R packages / functions that will make your life a lot easier. Statistics that we will cover include: mean median standard deviation interquartile range skewness Mean vs. median As you will know, the mean and the median are two very different statistics. The first is the average value of a data sample, i.e. \\(\\mu = 1/n \\sum y_i\\) (with \\(y_i\\) being individual data points or values of your sample), whereas the median separates the lower half and upper half of a data sample (i.e. is the value in the middle if you were to order all values form smallest to biggest). How much these two differ, however, depends on how the data is distributed. To demonstrates this, let’s calculate both and compare for different distributions # sample based on normal distribution normSample &lt;- rnorm(1000, mean = 10, sd = 1) print(c(mean(normSample), median(normSample))) ## [1] 9.965866 9.972984 # sample based on normal distribution poisSample &lt;- rpois(1000, lambda = 10) print(c(mean(poisSample), median(poisSample))) ## [1] 9.944 10.000 # sample based on normal distribution expSample &lt;- rexp(1000, rate = 0.1) print(c(mean(expSample), median(expSample))) ## [1] 9.583209 6.828290 We can see that for the first two samples, both the mean and median are fairly similar. For the sample based on an exponential distribution, however, they are very different. Therefore, whether the mean is an adequate representation (or statistics) of your data crucially depends on how the data is distributed! Standard deviation The standard deviation (SD) is a measure of the dispersal of a set of values and how much these values vary from the sample mean. A low standard variation means that most values are close to the mean, whereas a high standard deviation means that values are spread far way from the mean (which in itself should give you an indication of the usefulness of using the mean to describe your samples in the first case). Mathematically, the standard deviation, usually denoted \\(\\sigma\\), is given as \\[ \\sigma = \\sqrt{ \\frac{1}{n} \\sum (y_i - \\mu)^2 } \\] However, it is much easier to understand this visually, here demonstrated by two normal distributions with the same mean but one with SD=1 and one with SD=4 data.frame(SD1 = rnorm(10000, sd = 1), SD4 = rnorm(10000, sd = 4)) %&gt;% ggplot() + geom_density(aes(x = SD1, fill = &#39;SD=1&#39;), alpha = 0.3) + geom_density(aes(x = SD4, fill = &#39;SD=4&#39;), alpha = 0.3) + geom_vline(xintercept = 0, col = &#39;blue&#39;, linetype = &#39;dashed&#39;) + labs(x = &#39;&#39;, fill = &#39;&#39;) + theme(legend.position = &#39;top&#39;) ## Warning: The `scale_name` argument of `discrete_scale()` is deprecated as of ggplot2 3.5.0. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. Interquartile range Another measure of the spread of data is the so-called interquartile range, or IQR for short. The IQR is defined as the difference between the 75th percentile (third quartile, or Q3) and the 25th percentiles of the data (lower quartile, or Q1). Therefore, the IQR = Q3 − Q1. In R we can get the IQR’s simply through the function IQR() # interquartile range of sample based on normal distribution IQR(normSample) ## [1] 1.375224 # interquartile range of sample based on Poisson distribution IQR(poisSample) ## [1] 4 # interquartile range of sample based on exponential distribution IQR(expSample) ## [1] 10.74354 IQR’s form the basis of box-and-whisker plots, where the boxes show the median (Q2) and interquartile range (Q1 and Q3) and the lower and upper whiskers are defined as \\(Q1 - 1.5 \\times IQR\\) and \\(Q3 + 1.5 \\times IQR\\), respectively. Data points outside these rangers are considered outliers. sampleDist &lt;- data.frame(normal = normSample, poisson = poisSample, exponential = expSample) ggplot(sampleDist) + geom_boxplot(aes(x = 1, y = normal, fill = &#39;normal&#39;)) + geom_boxplot(aes(x = 2, y = poisson, fill = &#39;Poisson&#39;)) + geom_boxplot(aes(x = 3, y = exponential, fill = &#39;exponential&#39;)) + scale_x_continuous(breaks = c(1, 2, 3), labels = c(&#39;Normal&#39;, &#39;Poisson&#39;, &#39;Exponential&#39;)) + labs(x = &#39;Distribution&#39;, y = &#39;Value&#39;) + theme(legend.position = &#39;&#39;, text = element_text(size=14)) Skewness Skewness tells us whether the data is symmetric or asymmetrically distributed, an we can distinguish between a positive skew, where the right tail of the distribution is longer, and negative skew, where the left tail is longer. Let’s have a look at our three samples based on the normal, Poisson and exponential distribution: sampleDist %&gt;% gather(distribution, value, normal:exponential) %&gt;% ggplot(aes(x = value, fill = distribution)) + geom_density() + facet_wrap(~distribution, scales = &#39;free&#39;) + geom_vline(xintercept = 10, col = &#39;blue&#39;, linetype = &#39;dashed&#39;) + labs(x = &#39;&#39;, fill = &#39;&#39;) + theme(legend.position = &#39;&#39;) What is clear is that the exponential distribution has a significant positive skew. The R library moments provides useful functions to measure the shape of a distribution. Let’s use this here and see whether it agrees with our assessment by eye # if you do not have this library, install it with install.packages(&quot;moments&quot;) library(moments) # calculate skewness of normal-based samples skewness(normSample) ## [1] -0.02358933 # calculate skewness of normal-based samples skewness(poisSample) ## [1] 0.2504185 # calculate skewness of normal-based samples skewness(expSample) ## [1] 1.793003 The results are very much as expected, with the sample based on the exponential distribution being by far the most skewed. And this offers one explanation for why the mean and median differs so much: although the bulk of the data lies on the left hand side, the long tail to the right has a significant effect on the mean, such that it is no longer representative of the average value of a data sampled from this distribution (i.e. most values would be smaller than the mean). Note, although the skewness for the samples based on the normal distribution is also positive, the value is very small. The same goes for the ones based on the Poisson distribution. What shall we do with this information? I.e. is there evidence that the they are all skewed and hence sufficiently different from a normal distribution? Thankfully the moments package provides us with the tool to check this statistically based on kurtosis, which is a measure of how much of the data is found in the tails of the distribution. Kurtosis The kurtosis of a distribution is a measure of whether or not it is heavy-tailed or light-tailed relative to a normal distribution: the kurtosis of a normal distribution is 3 if the kurtosis is &lt;3, this means that it has fewer and less extreme outliers than the normal distribution if the kurtosis is &gt;3, this means that it has more outliers than the normal distribution. Note: some formulars substract 3 from the kurtosis, resulting in either negative or positive values (with the normal distribution being 0) The moments package also provides the functionality to calculate the kurtosis of a data sample. For our data this looks like # calculate skewness of normal-based samples kurtosis(normSample) ## [1] 2.922701 # calculate skewness of normal-based samples kurtosis(poisSample) ## [1] 2.905605 # calculate skewness of normal-based samples kurtosis(expSample) ## [1] 7.846492 Once again, the last data sample stands out by having a kurtosis much greater than 3, meaning it has far more outliers than would be expected if the data came from a normal distribution (as can be seen in the boxplot above, where outliers are indicated by open circles). As mentioned earlier, moments offers a statistical test (the so-called Jarque-Bera Normality Test), which compare a given data sample to a normal distribution, with the Null Hypothesis (H\\(_0\\)) being that the data has a skewness and kurtosis that matches a normal distribution. Let’s put this to the test for our three samples. # samples based on normal distribution jarque.test(normSample) ## ## Jarque-Bera Normality Test ## ## data: normSample ## JB = 0.3417, p-value = 0.8429 ## alternative hypothesis: greater # samples based on Poisson distribution jarque.test(poisSample) ## ## Jarque-Bera Normality Test ## ## data: poisSample ## JB = 10.823, p-value = 0.004465 ## alternative hypothesis: greater # samples based on exponential distribution jarque.test(expSample) ## ## Jarque-Bera Normality Test ## ## data: expSample ## JB = 1514.5, p-value &lt; 2.2e-16 ## alternative hypothesis: greater And this provides evidence that the data samples based on the Poisson and exponential distribution are statistically different from the normal distribution. Data summaries The reason we care about univariate statistics is that most downstream analyses and statistical tests intrinsically rely on them. For example, we often report on the mean of some numeric variable, such as age; however, as we have seen, the mean is not a good representative in case the data is very skewed, in which case the median is a better statistic. Comparing two populations is often done using a t-test, which not only compares their means but also makes very strong assumptions about the distribution. It is therefore crucial to get a good overview of our data before we start with the analyses. As we have seen, getting summary statistics by hand is relatively easy but can become cumbersome for large datasets with many variables. Thankfully, there are plenty of R functions out there that make our lives a lot easier. First, we are going to use inbuilt functions (that do not rely on external libraries) and then briefly introduce the summarytools package. One of the easiest way to get a summary overview of the data is to use the inbuilt summary() function ## remember to read in the data, in case you start a new session # tobacco &lt;- read_csv(&#39;tobacco.csv&#39;) %&gt;% # mutate(gender = factor(gender), # age.gr = factor(age.gr), # smoker = factor(smoker), # diseased = factor(diseased), # disease = factor(disease)) summary(tobacco) ## gender age age.gr BMI smoker ## F :489 Min. :18.0 18-34:258 Min. : 8.826 No :702 ## M :489 1st Qu.:34.0 35-50:241 1st Qu.:22.927 Yes:298 ## NA&#39;s: 22 Median :50.0 51-70:317 Median :25.620 ## Mean :49.6 71 + :159 Mean :25.731 ## 3rd Qu.:66.0 NA&#39;s : 25 3rd Qu.:28.649 ## Max. :80.0 Max. :39.439 ## NA&#39;s :25 NA&#39;s :26 ## cigs.per.day diseased disease samp.wgts ## Min. : 0.000 No :776 Hypertension: 36 Min. :0.8614 ## 1st Qu.: 0.000 Yes:224 Cancer : 34 1st Qu.:0.8614 ## Median : 0.000 Cholesterol : 21 Median :1.0442 ## Mean : 6.782 Heart : 20 Mean :1.0000 ## 3rd Qu.:11.000 Pulmonary : 20 3rd Qu.:1.0494 ## Max. :40.000 (Other) : 91 Max. :1.0625 ## NA&#39;s :35 NA&#39;s :778 This summary provides us with some of the most important information about our data. Besides revealing the different data types and data ranges, plus summary statistics, we can also get an idea how much data is missing for what variable. Furthermore, for categorical variables we also get information about how many observations fall into the different categories. Regarding categorical variables, we are often interested how these are distributed, not just across the whole data but also with respect to other categorical variables, in which case we refer to cross-tabulation or contingency tables. Even though this is not strictly part of univariate analysis, it makes sense to show how we can use the inbuilt table() function to easily tabulate categorical data. ## create a table based on individuals&#39; disease status table(tobacco$diseased) ## ## No Yes ## 776 224 ## create a contingency table of disease status stratified by gender + add cat names table(Diseased = tobacco$diseased, Gender = tobacco$gender) ## Gender ## Diseased F M ## No 378 379 ## Yes 111 110 We can go one step further and obtain stratified summary statistics using the inbuilt aggregate() function, which we here use to get the mean age and mean BMI stratified by gender and disease status aggregate(cbind(age, BMI) ~ gender + diseased, data = tobacco, mean ) ## gender diseased age BMI ## 1 F No 48.59270 25.91337 ## 2 M No 48.57300 25.11816 ## 3 F Yes 52.95283 26.81004 ## 4 M Yes 53.60577 26.01160 Based on this table we would conclude that diseased individuals are on average older and have a higher BMI than disease-free individuals and with not discernible difference between males and females. In later sections we will learn hot to test this statistically. Before moving on it might be worth it to show how the same can also be achieved using dplyr (part of the tidyverse) tobacco %&gt;% filter(!is.na(age) &amp; !is.na(BMI) &amp; !is.na(gender) &amp; !is.na(diseased)) %&gt;% group_by(gender, diseased) %&gt;% summarise(mean_age = mean(age), mean_BMI = mean(BMI)) ## # A tibble: 4 × 4 ## # Groups: gender [2] ## gender diseased mean_age mean_BMI ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F No 48.6 25.9 ## 2 F Yes 53.0 26.8 ## 3 M No 48.6 25.1 ## 4 M Yes 53.6 26.0 Question What does filter(!is.na(age) &amp; !is.na(BMI) &amp; !is.na(gender) &amp; !is.na(diseased)) achieve, and what happens is we run the code without? To finish off this section we briefly introduce the summarytools package, which provides a number of useful tools to help us to summarise our data. The first tool, or function, is dfsummary(), which can be seen as an extension to the inbuilt summary() and provides us with almost everything we need in one go. # load the library library(summarytools) # get a summary of our dataframe dfSummary(tobacco) ## Data Frame Summary ## tobacco ## Dimensions: 1000 x 9 ## Duplicates: 2 ## ## -------------------------------------------------------------------------------------------------------------- ## No Variable Stats / Values Freqs (% of Valid) Graph Valid Missing ## ---- -------------- ------------------------- --------------------- --------------------- ---------- --------- ## 1 gender 1. F 489 (50.0%) IIIIIIIIII 978 22 ## [factor] 2. M 489 (50.0%) IIIIIIIIII (97.8%) (2.2%) ## ## 2 age Mean (sd) : 49.6 (18.3) 63 distinct values . . . . . : 975 25 ## [numeric] min &lt; med &lt; max: : : : : : . : : : : (97.5%) (2.5%) ## 18 &lt; 50 &lt; 80 : : : : : : : : : : ## IQR (CV) : 32 (0.4) : : : : : : : : : : ## : : : : : : : : : : ## ## 3 age.gr 1. 18-34 258 (26.5%) IIIII 975 25 ## [factor] 2. 35-50 241 (24.7%) IIII (97.5%) (2.5%) ## 3. 51-70 317 (32.5%) IIIIII ## 4. 71 + 159 (16.3%) III ## ## 4 BMI Mean (sd) : 25.7 (4.5) 974 distinct values : 974 26 ## [numeric] min &lt; med &lt; max: : : : (97.4%) (2.6%) ## 8.8 &lt; 25.6 &lt; 39.4 : : : ## IQR (CV) : 5.7 (0.2) : : : : : ## . : : : : : . ## ## 5 smoker 1. No 702 (70.2%) IIIIIIIIIIIIII 1000 0 ## [factor] 2. Yes 298 (29.8%) IIIII (100.0%) (0.0%) ## ## 6 cigs.per.day Mean (sd) : 6.8 (11.9) 37 distinct values : 965 35 ## [integer] min &lt; med &lt; max: : (96.5%) (3.5%) ## 0 &lt; 0 &lt; 40 : ## IQR (CV) : 11 (1.8) : ## : . . . . . . ## ## 7 diseased 1. No 776 (77.6%) IIIIIIIIIIIIIII 1000 0 ## [factor] 2. Yes 224 (22.4%) IIII (100.0%) (0.0%) ## ## 8 disease 1. Cancer 34 (15.3%) III 222 778 ## [factor] 2. Cholesterol 21 ( 9.5%) I (22.2%) (77.8%) ## 3. Diabetes 14 ( 6.3%) I ## 4. Digestive 12 ( 5.4%) I ## 5. Hearing 14 ( 6.3%) I ## 6. Heart 20 ( 9.0%) I ## 7. Hypertension 36 (16.2%) III ## 8. Hypotension 11 ( 5.0%) ## 9. Musculoskeletal 19 ( 8.6%) I ## 10. Neurological 10 ( 4.5%) ## [ 3 others ] 31 (14.0%) II ## ## 9 samp.wgts Mean (sd) : 1 (0.1) 0.86!: 267 (26.7%) IIIII 1000 0 ## [numeric] min &lt; med &lt; max: 1.04!: 249 (24.9%) IIII (100.0%) (0.0%) ## 0.9 &lt; 1 &lt; 1.1 1.05!: 324 (32.4%) IIIIII ## IQR (CV) : 0.2 (0.1) 1.06!: 160 (16.0%) III ## ! rounded ## -------------------------------------------------------------------------------------------------------------- The next function we introduce, ctable(). is an extension to the table() function mentioned earlier ctable(tobacco$smoker, tobacco$diseased) ## Cross-Tabulation, Row Proportions ## smoker * diseased ## Data Frame: tobacco ## ## -------- ---------- ------------- ------------- --------------- ## diseased No Yes Total ## smoker ## No 603 (85.9%) 99 (14.1%) 702 (100.0%) ## Yes 173 (58.1%) 125 (41.9%) 298 (100.0%) ## Total 776 (77.6%) 224 (22.4%) 1000 (100.0%) ## -------- ---------- ------------- ------------- --------------- This does not seem to offer much more, except that we can easily ask it to run a statistical test (\\(\\chi^2\\)-test) on the resulting table ctable(tobacco$smoker, tobacco$diseased, chisq = T) ## Cross-Tabulation, Row Proportions ## smoker * diseased ## Data Frame: tobacco ## ## ## -------- ---------- ------------- ------------- --------------- ## diseased No Yes Total ## smoker ## No 603 (85.9%) 99 (14.1%) 702 (100.0%) ## Yes 173 (58.1%) 125 (41.9%) 298 (100.0%) ## Total 776 (77.6%) 224 (22.4%) 1000 (100.0%) ## -------- ---------- ------------- ------------- --------------- ## ## ---------------------------- ## Chi.squared df p.value ## ------------- ---- --------- ## 91.7088 1 0 ## ---------------------------- Without going into much of the details at this point, the p.value of 0 indicates that there is strong statistical evidence that disease status is dependent on smoking - pretty much as we would expect. A slightly more useful addition is when we combine ctable with stby, which allows us to create contingency tables with respect to a third categorical variable; for example stby(list(tobacco$diseased, tobacco$gender), tobacco$smoker, ctable) ## Cross-Tabulation, Row Proportions ## diseased * gender ## Data Frame: tobacco ## Group: smoker = No ## ## ---------- -------- ------------- ------------- ----------- -------------- ## gender F M &lt;NA&gt; Total ## diseased ## No 293 (48.6%) 299 (49.6%) 11 (1.8%) 603 (100.0%) ## Yes 49 (49.5%) 47 (47.5%) 3 (3.0%) 99 (100.0%) ## Total 342 (48.7%) 346 (49.3%) 14 (2.0%) 702 (100.0%) ## ---------- -------- ------------- ------------- ----------- -------------- ## ## Group: smoker = Yes ## ## ---------- -------- ------------- ------------- ---------- -------------- ## gender F M &lt;NA&gt; Total ## diseased ## No 85 (49.1%) 80 (46.2%) 8 (4.6%) 173 (100.0%) ## Yes 62 (49.6%) 63 (50.4%) 0 (0.0%) 125 (100.0%) ## Total 147 (49.3%) 143 (48.0%) 8 (2.7%) 298 (100.0%) ## ---------- -------- ------------- ------------- ---------- -------------- Finally we are going to mention the tableby() function of the arsenal package, which is a highly customisable and powerful tool to provide detailed and publication-ready data summaries (more information can be found here). #load required library library(arsenal) # create a simple table tab1 &lt;- tableby(diseased ~ gender + smoker + BMI + age, data=tobacco) summary(tab1) No (N=776) Yes (N=224) Total (N=1000) p value gender 0.939    N-Miss 19 3 22    F 378 (49.9%) 111 (50.2%) 489 (50.0%)    M 379 (50.1%) 110 (49.8%) 489 (50.0%) smoker &lt; 0.001    No 603 (77.7%) 99 (44.2%) 702 (70.2%)    Yes 173 (22.3%) 125 (55.8%) 298 (29.8%) BMI 0.010    N-Miss 22 4 26    Mean (SD) 25.532 (4.448) 26.414 (4.567) 25.731 (4.488)    Range 8.826 - 39.439 10.346 - 39.208 8.826 - 39.439 age 0.002    N-Miss 18 7 25    Mean (SD) 48.609 (18.019) 53.069 (18.839) 49.602 (18.289)    Range 18.000 - 80.000 18.000 - 80.000 18.000 - 80.000 2.2 Multivariate analysis In a multivariate analysis we are usually interested in the relationship between pairs of variables. Here we will briefly go through three commonly used methods to examine how two or more variables are related: contingency tables, which we have already come across in the previous section Pearson correlation Spearman rank correlation Contingency tables Contingency tables are types of tables that display the frequency distribution of two variables against each other. As an example, say we had data from a clinical trial where patients were given either a treatment or a placebo and we are interested in how many people recovered from a disease within 5 days. In each arm we had 100 individuals and in the treatment group 73 individuals recovered and in the placebo group 64. In table format this would thus look like this teatment recovered disease drug A 73 27 placebo 64 36 We can see that there were more individuals who recovered in the treatment arm of the study. But how do we know that this was not due to chance? To answer this question we have to briefly tap into inferential statistics. And two common methods to provide functions to perform statistical tests on contingency tables: Pearson’s chi-squared test and Fisher’s exact test. We are not going into the details of where they differ but only mention that Fisher’s exact test is non-parametric, typically defined on a 2 x 2 contingency table, and, importantly, works with small sample sizes. The chi-squared (or \\(\\chi^2\\)) test, on the other hand, works on more than one variables but usualy requires larger sample sizes. Pearson’s chi-squared test Perform a chi-squared test in R is straightforward using the chisq.test() function. # define our contingency table trial &lt;- data.frame(recovered = c(73, 64), disease = c(27, 36)) # add row names (not necessary) row.names(trial) &lt;- c(&#39;drug A&#39;, &#39;placebo&#39;) # run test chisq.test(trial) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: trial ## X-squared = 1.483, df = 1, p-value = 0.2233 Because data can come in different formats, here we provide an example of how to create a simple contingency table if your data only had the recorded outcome, for example as recovered / not recovered or recovered yes / no, stored in two columns, one for the treatment arm and one for the placebo arm. # recorded outcome recovered yes / no trialData &lt;- data.frame(drug = c(rep(&#39;recovered&#39;,73), rep(&#39;not recovered&#39;, 27)), placebo = c(rep(&#39;recovered&#39;,64), rep(&#39;not recovered&#39;, 36))) # first turn into &quot;tidy&quot; format trialData &lt;- trialData %&gt;% gather(treatment, outcome, drug:placebo) # create a contingency table contTable &lt;- table(trialData) # perform chi-sq test chisq.test(contTable) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: contTable ## X-squared = 1.483, df = 1, p-value = 0.2233 Fisher’s exact test Fisher’s exact test work in a very similar way and directly on a 2 x 2 contingency table. For large sample sizes you will notice that both test give you similar test statistics, but as mentioned, it is more powerful when sample sizes are small. # run Fisher&#39;s exact test on previously defined contingency matrix fisher.test(contTable) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: contTable ## p-value = 0.2232 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3438828 1.2516078 ## sample estimates: ## odds ratio ## 0.6589328 As you will have noticed, this test works on and also reports odds ratio, which is a handy “side effect” of using this function. Pearson correlation The aim of the Pearson correlation coefficient, or most commonly referred to simply as correlation coefficient, is to establish a line of best fit through a dataset of two variables and measure how far away the data are from the expected values (the best fit line). Values range between +1 (perfect positive correlation) to -1 (perfect negative correlation), and any value in between. Here are some examples ## `geom_smooth()` using formula = &#39;y ~ x&#39; To calculate the correlation coefficient in R between two vectors \\(x\\) and \\(y\\) we simply call the cor(x,y) function. And if we are further interested in whether the correlation (or lack of it) is statistically significant we can use the cor.test(x,y) function. Task Create a data.frame with two simulated (artifical) data samples. Plot this data using ggplot and add a linear regression line (remember geom_smooth()?). Then test whether there is a correlation between the two variables and if so, test whether this is statistically significant. Show: Solution Solution # sample from uniform distribution x &lt;- runif(50, 1, 5) # assume linear relationship between x and y plus some Gaussian noise y &lt;- 2.3*x + rnorm(50,0,1) # create data.frame df &lt;- data.frame(x = x, y = y) # plot ggplot(df, aes(x, y)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se=F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # test for correlation cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 19.98, df = 48, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9042758 0.9684661 ## sample estimates: ## cor ## 0.9448115 \\(~\\) ### Spearman’s rank correlation {-} In comparison to Pearson’s correlation, Spearman’s rank correlation is a non-parametric measure of how the ranking of two variables are correlated. In fact, the Spearman correlation is equal to the Pearson correlation between the rank values of two variables. So instead of comparing the values, here we compare their ranks, or their indices when ordered from smallest to largest. As before, the values for Spearman’s rho (\\(\\rho\\)) can range from -1 to +1. Without providing any more detail, here is an example of how to calculate Spearman’s rho in R Age &lt;- c(9, 11, 1, 7, 6, 5, 10, 3, 4, 4) OD &lt;- c(478, 755, 399, 512, 458, 444, 716, 399, 491, 456) # use the same function as before but define method = &#39;spearman&#39; cor(Age, OD, method = &#39;spearman&#39;) ## [1] 0.8597561 # and test for significance cor.test(Age, OD, method = &#39;spearman&#39;) ## Warning in cor.test.default(Age, OD, method = &quot;spearman&quot;): Cannot compute exact ## p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: Age and OD ## S = 23.14, p-value = 0.001424 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8597561 Task Compare the two correlation coefficients for different data. Correlation vs. causation Beware: even a very high correlation between two variables does not infer causality - causality can only be inferred through careful experimental design in a well-controlled setting. A good example how looking purely at correlation can be misleading is this one (taken from Spurious Correlations) \\(\\rho = 0.947\\)! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
